import pandas as pd
import numpy as np
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
import kagglehub

# Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ°
path = kagglehub.dataset_download("fedesoriano/stroke-prediction-dataset")
df = pd.read_csv(f"{path}/healthcare-dataset-stroke-data.csv")

# ÐšÐ°Ñ‚ÐµÐ³Ð¾Ñ€Ð¸Ð°Ð»ÑŒÐ½Ñ‹Ðµ Ð¿ÐµÑ€ÐµÐ¼ÐµÐ½Ð½Ñ‹Ðµ
map_bin = {
    'gender': {'Male': 1, 'Female': 0},
    'ever_married': {'Yes': 1, 'No': 0},
    'Residence_type': {'Urban': 1, 'Rural': 0},
    'smoking_status': {'never smoked': 0, 'formerly smoked': 0.5, 'smokes': 1}
}
for col, mapping in map_bin.items():
    if col in df.columns:
        df[col] = df[col].map(mapping)

df = pd.get_dummies(df, columns=['work_type'], prefix='work')
df = df[df['stroke'].notna()]
df['stroke'] = df['stroke'].astype(int)

# Ð˜Ð¼Ð¿ÑƒÑ‚Ð°Ñ†Ð¸Ñ
imputer = IterativeImputer(random_state=42)
num_cols = df.select_dtypes(include=['float64', 'int64']).columns.drop('stroke')
df[num_cols] = imputer.fit_transform(df[num_cols])

# Ð”ÐµÐ»ÐµÐ½Ð¸Ðµ: 70 / 10 / 10 / 10
X = df.drop(columns=['stroke'])
y = df['stroke']
X_train, X_rest, y_train, y_rest = train_test_split(X, y, test_size=0.3, stratify=y, random_state=43)
X_corr1, X_rest, y_corr1, y_rest = train_test_split(X_rest, y_rest, test_size=2/3, stratify=y_rest, random_state=42)
X_corr2, X_val, y_corr2, y_val = train_test_split(X_rest, y_rest, test_size=0.5, stratify=y_rest, random_state=42)

# ÐœÐ°ÑÑˆÑ‚Ð°Ð±Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ
scaler = StandardScaler()
X_train_s = scaler.fit_transform(X_train)
X_corr1_s = scaler.transform(X_corr1)
X_corr2_s = scaler.transform(X_corr2)
X_val_s = scaler.transform(X_val)

# ÐœÐ¾Ð´ÐµÐ»ÑŒ main
main = RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=0)
main.fit(X_train_s, y_train)

# correction 1
p_corr1 = main.predict_proba(X_corr1_s)[:, 1]
y_corr1_pred = (p_corr1 > 0.25).astype(int)
fp1 = (y_corr1_pred == 1) & (y_corr1 == 0)
fn1 = (y_corr1_pred == 0) & (y_corr1 == 1)

X_c1 = pd.concat([X_corr1[fp1], X_corr1[fn1]])
y_c1 = pd.concat([y_corr1[fp1], y_corr1[fn1]])
model_c1 = RandomForestClassifier(n_estimators=100, random_state=1)
model_c1.fit(scaler.transform(X_c1), y_c1)

# correction fnx (Ð½Ð° false negatives Ð¸ real 0 Ð² ÑÐ¾Ð¾Ñ‚Ð½Ð¾ÑˆÐµÐ½Ð¸Ð¸ 1:2)
nonstroke_pool_fnx = pd.concat([X_train[y_train == 0], X_corr1[y_corr1 == 0]])
n_fnx = fn1.sum()
n_zeros = min(n_fnx * 2, len(nonstroke_pool_fnx))

if n_fnx > 0 and n_zeros > 0:
    nonstroke_sample_fnx = nonstroke_pool_fnx.sample(n=n_zeros, random_state=99)
    X_fnx = pd.concat([X_corr1[fn1], nonstroke_sample_fnx])
    y_fnx = pd.Series([1] * n_fnx + [0] * n_zeros)
    model_fnx = RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=99)
    model_fnx.fit(scaler.transform(X_fnx), y_fnx)
    px = model_fnx.predict_proba(X_val_s)[:, 1]
else:
    print("âš ï¸ FNX skipped due to lack of samples.")
    px = np.zeros(len(X_val_s))

# correction 2
p_corr2 = model_c1.predict_proba(X_corr2_s)[:, 1]
y_corr2_pred = (p_corr2 > 0.25).astype(int)
fp2 = (y_corr2_pred == 1) & (y_corr2 == 0)
stroke_pool = pd.concat([X_train[y_train == 1], X_corr1[y_corr1 == 1]])
stroke_sample2 = stroke_pool.sample(n=min(fp2.sum(), len(stroke_pool)), random_state=2)
X_c2 = pd.concat([X_corr2[fp2], stroke_sample2])
y_c2 = pd.Series([0] * fp2.sum() + [1] * len(stroke_sample2))
model_c2 = RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=2)
model_c2.fit(scaler.transform(X_c2), y_c2)

# correction 3
p_corr3 = model_c2.predict_proba(X_val_s)[:, 1]
y_corr3_pred = (p_corr3 > 0.25).astype(int)
fp3 = (y_corr3_pred == 1) & (y_val == 0)
stroke_pool3 = pd.concat([X_train[y_train == 1], X_corr2[y_corr2 == 1]])
stroke_sample3 = stroke_pool3.sample(n=min(fp3.sum(), len(stroke_pool3)), random_state=2)
X_c3 = pd.concat([X_val[fp3], stroke_sample3])
y_c3 = pd.Series([0] * fp3.sum() + [1] * len(stroke_sample3))
model_c3 = RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=3)
model_c3.fit(scaler.transform(X_c3), y_c3)

# Ñ„Ð¸Ð½Ð°Ð»ÑŒÐ½Ð°Ñ ÐºÐ¾Ñ€Ñ€ÐµÐºÑ‚Ð¸Ñ€ÑƒÑŽÑ‰Ð°Ñ Ð¼Ð¾Ð´ÐµÐ»ÑŒ (Ð½Ð° Ð²ÑÐµÑ… Ð¾ÑˆÐ¸Ð±ÐºÐ°Ñ…)
all_fp = pd.concat([X_corr1[fp1], X_corr2[fp2], X_val[fp3]])
all_fn = pd.concat([X_corr1[fn1]])
false_all = pd.concat([all_fp, all_fn])
n_false = len(false_all)

n_half_1 = min(n_false // 2, len(X_train[y_train == 1]))
n_half_0 = min(n_false - n_half_1, len(X_train[y_train == 0]))

real_balanced = pd.concat([
    X_train[y_train == 1].sample(n=n_half_1, random_state=7),
    X_train[y_train == 0].sample(n=n_half_0, random_state=8)
])

X_final_corr = pd.concat([false_all, real_balanced])
y_final_corr = pd.Series([0] * len(false_all) + [1] * len(real_balanced))

model_final_corr = RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=100)
model_final_corr.fit(scaler.transform(X_final_corr), y_final_corr)
p_final_corr = model_final_corr.predict_proba(X_val_s)[:, 1]


# Ð¤Ð¸Ð½Ð°Ð»ÑŒÐ½Ð°Ñ Ð²Ð°Ð»Ð¸Ð´Ð°Ñ†Ð¸Ñ
p_main = (main.predict_proba(X_val_s)[:, 1] > 0.6).astype(int)
p1 = model_c1.predict_proba(X_val_s)[:, 1]
p2 = model_c2.predict_proba(X_val_s)[:, 1]
p3 = model_c3.predict_proba(X_val_s)[:, 1]

# Ð“Ð¾Ð»Ð¾ÑÐ¾Ð²Ð°Ð½Ð¸Ðµ
ensemble = (
    0.55 * p_main +
    0.12 * p1 +
    0.02 * px +
    0.15 * p2 +
    0.15 * p3 +
    0.0 * p_final_corr  # Ð¿Ð¾ÐºÐ° Ð±ÐµÐ· Ð²Ð»Ð¸ÑÐ½Ð¸Ñ
)
y_pred = (ensemble > 0.245).astype(int)

# ÐœÐµÑ‚Ñ€Ð¸ÐºÐ¸
print("ðŸŽ¯ Accuracy:", accuracy_score(y_val, y_pred))
print("\nðŸ“Š Classification Report:\n", classification_report(y_val, y_pred, digits=4))
print("ðŸ§¾ Confusion Matrix:\n", confusion_matrix(y_val, y_pred))



